#!/bin/bash
# Script de Testes - Bot de Trading MVP
# Este script automatiza a execu√ß√£o de todos os tipos de testes

set -e  # Parar em caso de erro

# Configura√ß√µes
PROJECT_NAME="trading-bot-mvp"
TEST_DB_NAME="trading_bot_test"
COVERAGE_THRESHOLD=70

# Cores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Fun√ß√£o para log
log() {
    echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}"
}

success() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] ‚úÖ $1${NC}"
}

error() {
    echo -e "${RED}[$(date +'%Y-%m-%d %H:%M:%S')] ‚ùå $1${NC}"
}

warning() {
    echo -e "${YELLOW}[$(date +'%Y-%m-%d %H:%M:%S')] ‚ö†Ô∏è  $1${NC}"
}

info() {
    echo -e "${CYAN}[$(date +'%Y-%m-%d %H:%M:%S')] ‚ÑπÔ∏è  $1${NC}"
}

# Fun√ß√£o para verificar depend√™ncias
check_dependencies() {
    log "Verificando depend√™ncias para testes..."
    
    # Verificar Python
    if ! command -v python3 &> /dev/null; then
        error "Python 3 n√£o encontrado!"
        exit 1
    fi
    
    # Verificar se estamos em um ambiente virtual
    if [[ "$VIRTUAL_ENV" == "" ]]; then
        warning "N√£o est√° em um ambiente virtual"
        if [ -d "venv" ]; then
            log "Ativando ambiente virtual..."
            source venv/bin/activate
        else
            warning "Ambiente virtual n√£o encontrado - criando..."
            python3 -m venv venv
            source venv/bin/activate
        fi
    fi
    
    # Verificar pytest
    if ! python -c "import pytest" 2>/dev/null; then
        log "Instalando pytest..."
        pip install pytest pytest-cov pytest-asyncio pytest-mock pytest-xdist
    fi
    
    success "Depend√™ncias verificadas"
}

# Fun√ß√£o para configurar ambiente de teste
setup_test_environment() {
    log "Configurando ambiente de teste..."
    
    # Carregar vari√°veis de ambiente de teste
    if [ -f ".env.test" ]; then
        export $(cat .env.test | grep -v '^#' | xargs)
        info "Vari√°veis de .env.test carregadas"
    elif [ -f ".env" ]; then
        export $(cat .env | grep -v '^#' | xargs)
        info "Vari√°veis de .env carregadas"
    fi
    
    # Configurar vari√°veis espec√≠ficas para teste
    export ENVIRONMENT=test
    export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/${TEST_DB_NAME}"
    export REDIS_URL="redis://localhost:6379/1"
    export BYBIT_TESTNET=true
    export LOG_LEVEL=WARNING
    
    # Verificar se PostgreSQL est√° rodando
    if command -v pg_isready &> /dev/null; then
        if pg_isready -h localhost -p 5432 &> /dev/null; then
            success "PostgreSQL est√° rodando"
        else
            warning "PostgreSQL n√£o est√° rodando - alguns testes podem falhar"
        fi
    fi
    
    # Verificar se Redis est√° rodando
    if command -v redis-cli &> /dev/null; then
        if redis-cli ping &> /dev/null; then
            success "Redis est√° rodando"
        else
            warning "Redis n√£o est√° rodando - alguns testes podem falhar"
        fi
    fi
    
    success "Ambiente de teste configurado"
}

# Fun√ß√£o para preparar banco de dados de teste
setup_test_database() {
    log "Preparando banco de dados de teste..."
    
    # Verificar se psql est√° dispon√≠vel
    if command -v psql &> /dev/null; then
        # Criar banco de teste se n√£o existir
        psql -h localhost -U postgres -tc "SELECT 1 FROM pg_database WHERE datname = '${TEST_DB_NAME}'" | grep -q 1 || \
        psql -h localhost -U postgres -c "CREATE DATABASE ${TEST_DB_NAME};"
        
        # Executar migrations se existir alembic
        if [ -f "alembic.ini" ]; then
            log "Executando migrations de teste..."
            alembic upgrade head
        fi
        
        success "Banco de dados de teste preparado"
    else
        warning "psql n√£o encontrado - pulando configura√ß√£o do banco"
    fi
}

# Fun√ß√£o para executar testes unit√°rios
run_unit_tests() {
    log "Executando testes unit√°rios..."
    
    if [ -d "tests/unit" ]; then
        pytest tests/unit/ \
            -v \
            --tb=short \
            --cov=. \
            --cov-report=term-missing \
            --cov-report=html:htmlcov/unit \
            --cov-report=xml:coverage-unit.xml \
            --junit-xml=test-results-unit.xml \
            --cov-fail-under=${COVERAGE_THRESHOLD} \
            -x
        
        success "Testes unit√°rios conclu√≠dos"
    else
        warning "Diret√≥rio tests/unit/ n√£o encontrado"
    fi
}

# Fun√ß√£o para executar testes de integra√ß√£o
run_integration_tests() {
    log "Executando testes de integra√ß√£o..."
    
    if [ -d "tests/integration" ]; then
        pytest tests/integration/ \
            -v \
            --tb=short \
            --junit-xml=test-results-integration.xml \
            -x
        
        success "Testes de integra√ß√£o conclu√≠dos"
    else
        warning "Diret√≥rio tests/integration/ n√£o encontrado"
    fi
}

# Fun√ß√£o para executar testes de API
run_api_tests() {
    log "Executando testes de API..."
    
    if [ -d "tests/api" ]; then
        # Verificar se a API est√° rodando
        if curl -f http://localhost:8000/health &> /dev/null; then
            pytest tests/api/ \
                -v \
                --tb=short \
                --junit-xml=test-results-api.xml
            
            success "Testes de API conclu√≠dos"
        else
            warning "API n√£o est√° rodando - pulando testes de API"
            info "Para executar testes de API, inicie a aplica√ß√£o primeiro"
        fi
    else
        warning "Diret√≥rio tests/api/ n√£o encontrado"
    fi
}

# Fun√ß√£o para executar testes de performance
run_performance_tests() {
    log "Executando testes de performance..."
    
    if [ -d "tests/performance" ]; then
        pytest tests/performance/ \
            -v \
            --tb=short \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --junit-xml=test-results-performance.xml
        
        success "Testes de performance conclu√≠dos"
    else
        warning "Diret√≥rio tests/performance/ n√£o encontrado"
    fi
}

# Fun√ß√£o para executar testes de seguran√ßa
run_security_tests() {
    log "Executando testes de seguran√ßa..."
    
    # Bandit - an√°lise de seguran√ßa do c√≥digo
    if command -v bandit &> /dev/null; then
        log "Executando Bandit (an√°lise de seguran√ßa)..."
        bandit -r . -f json -o security-report.json || warning "Problemas de seguran√ßa encontrados"
    fi
    
    # Safety - verifica√ß√£o de vulnerabilidades em depend√™ncias
    if command -v safety &> /dev/null; then
        log "Executando Safety (vulnerabilidades em depend√™ncias)..."
        safety check --json --output safety-report.json || warning "Vulnerabilidades encontradas"
    fi
    
    # Semgrep - an√°lise est√°tica de c√≥digo
    if command -v semgrep &> /dev/null; then
        log "Executando Semgrep (an√°lise est√°tica)..."
        semgrep --config=auto --json --output=semgrep-report.json . || warning "Problemas encontrados pelo Semgrep"
    fi
    
    success "Testes de seguran√ßa conclu√≠dos"
}

# Fun√ß√£o para executar testes de carga b√°sicos
run_load_tests() {
    log "Executando testes de carga b√°sicos..."
    
    # Verificar se a API est√° rodando
    if curl -f http://localhost:8000/health &> /dev/null; then
        log "Executando teste de carga simples..."
        
        # Teste b√°sico com curl
        for i in {1..50}; do
            curl -s http://localhost:8000/api/v1/status > /dev/null &
        done
        wait
        
        # Se locust estiver dispon√≠vel, executar teste mais completo
        if command -v locust &> /dev/null && [ -f "tests/load/locustfile.py" ]; then
            log "Executando teste com Locust..."
            locust -f tests/load/locustfile.py --headless -u 10 -r 2 -t 30s --host=http://localhost:8000
        fi
        
        success "Testes de carga conclu√≠dos"
    else
        warning "API n√£o est√° rodando - pulando testes de carga"
    fi
}

# Fun√ß√£o para gerar relat√≥rios
generate_reports() {
    log "Gerando relat√≥rios de teste..."
    
    # Criar diret√≥rio de relat√≥rios
    mkdir -p reports/
    
    # Mover arquivos de relat√≥rio
    mv test-results-*.xml reports/ 2>/dev/null || true
    mv coverage*.xml reports/ 2>/dev/null || true
    mv *-report.json reports/ 2>/dev/null || true
    mv benchmark-results.json reports/ 2>/dev/null || true
    
    # Gerar relat√≥rio consolidado
    cat > reports/test-summary.md << EOF
# üìä Relat√≥rio de Testes - $(date)

## üìà Cobertura de C√≥digo
$(coverage report --skip-covered 2>/dev/null || echo "Relat√≥rio de cobertura n√£o dispon√≠vel")

## üß™ Resumo dos Testes
- **Testes Unit√°rios**: $(grep -c "testcase" reports/test-results-unit.xml 2>/dev/null || echo "N/A") testes
- **Testes de Integra√ß√£o**: $(grep -c "testcase" reports/test-results-integration.xml 2>/dev/null || echo "N/A") testes
- **Testes de API**: $(grep -c "testcase" reports/test-results-api.xml 2>/dev/null || echo "N/A") testes
- **Testes de Performance**: $(grep -c "testcase" reports/test-results-performance.xml 2>/dev/null || echo "N/A") testes

## üîí Seguran√ßa
- **Bandit**: $([ -f reports/security-report.json ] && echo "Executado" || echo "N√£o executado")
- **Safety**: $([ -f reports/safety-report.json ] && echo "Executado" || echo "N√£o executado")
- **Semgrep**: $([ -f reports/semgrep-report.json ] && echo "Executado" || echo "N√£o executado")

## üìÅ Arquivos Gerados
- Cobertura HTML: htmlcov/index.html
- Relat√≥rios XML: reports/
- Relat√≥rios de Seguran√ßa: reports/
EOF
    
    success "Relat√≥rios gerados em reports/"
}

# Fun√ß√£o para limpeza p√≥s-teste
cleanup() {
    log "Executando limpeza p√≥s-teste..."
    
    # Limpar cache de teste
    find . -name "*.pyc" -delete 2>/dev/null || true
    find . -name "__pycache__" -type d -exec rm -rf {} + 2>/dev/null || true
    rm -rf .pytest_cache/ 2>/dev/null || true
    
    # Limpar banco de teste se especificado
    if [ "$CLEANUP_DB" = "true" ] && command -v psql &> /dev/null; then
        psql -h localhost -U postgres -c "DROP DATABASE IF EXISTS ${TEST_DB_NAME};" 2>/dev/null || true
    fi
    
    success "Limpeza conclu√≠da"
}

# Fun√ß√£o para mostrar ajuda
show_help() {
    echo "Uso: $0 [op√ß√µes]"
    echo ""
    echo "Op√ß√µes:"
    echo "  -u, --unit           Executar apenas testes unit√°rios"
    echo "  -i, --integration    Executar apenas testes de integra√ß√£o"
    echo "  -a, --api           Executar apenas testes de API"
    echo "  -p, --performance   Executar apenas testes de performance"
    echo "  -s, --security      Executar apenas testes de seguran√ßa"
    echo "  -l, --load          Executar apenas testes de carga"
    echo "  -f, --fast          Executar testes r√°pidos (unit√°rios + integra√ß√£o)"
    echo "  -A, --all           Executar todos os tipos de teste (padr√£o)"
    echo "  -c, --coverage      Gerar apenas relat√≥rio de cobertura"
    echo "  -C, --cleanup-db    Limpar banco de dados ap√≥s testes"
    echo "  -v, --verbose       Modo verboso"
    echo "  -h, --help          Mostrar esta ajuda"
    echo ""
    echo "Exemplos:"
    echo "  $0                  # Executar todos os testes"
    echo "  $0 -u               # Apenas testes unit√°rios"
    echo "  $0 -f               # Testes r√°pidos"
    echo "  $0 -s -C            # Testes de seguran√ßa e limpar DB"
}

# Fun√ß√£o principal
main() {
    local run_unit=false
    local run_integration=false
    local run_api=false
    local run_performance=false
    local run_security=false
    local run_load=false
    local run_all=true
    local coverage_only=false
    local verbose=false
    
    # Processar argumentos
    while [[ $# -gt 0 ]]; do
        case $1 in
            -u|--unit)
                run_unit=true
                run_all=false
                shift
                ;;
            -i|--integration)
                run_integration=true
                run_all=false
                shift
                ;;
            -a|--api)
                run_api=true
                run_all=false
                shift
                ;;
            -p|--performance)
                run_performance=true
                run_all=false
                shift
                ;;
            -s|--security)
                run_security=true
                run_all=false
                shift
                ;;
            -l|--load)
                run_load=true
                run_all=false
                shift
                ;;
            -f|--fast)
                run_unit=true
                run_integration=true
                run_all=false
                shift
                ;;
            -A|--all)
                run_all=true
                shift
                ;;
            -c|--coverage)
                coverage_only=true
                shift
                ;;
            -C|--cleanup-db)
                export CLEANUP_DB=true
                shift
                ;;
            -v|--verbose)
                verbose=true
                export PYTEST_ARGS="-v -s"
                shift
                ;;
            -h|--help)
                show_help
                exit 0
                ;;
            *)
                error "Op√ß√£o desconhecida: $1"
                show_help
                exit 1
                ;;
        esac
    done
    
    log "Iniciando execu√ß√£o de testes do $PROJECT_NAME..."
    
    # Verificar se estamos no diret√≥rio correto
    if [ ! -f "requirements.txt" ] && [ ! -f "pyproject.toml" ]; then
        error "N√£o parece ser um projeto Python v√°lido"
        exit 1
    fi
    
    # Configurar ambiente
    check_dependencies
    setup_test_environment
    setup_test_database
    
    # Executar testes baseado nos argumentos
    if [ "$coverage_only" = true ]; then
        log "Gerando apenas relat√≥rio de cobertura..."
        coverage html
        coverage report
    elif [ "$run_all" = true ]; then
        log "Executando todos os tipos de teste..."
        run_unit_tests
        run_integration_tests
        run_api_tests
        run_performance_tests
        run_security_tests
        run_load_tests
    else
        [ "$run_unit" = true ] && run_unit_tests
        [ "$run_integration" = true ] && run_integration_tests
        [ "$run_api" = true ] && run_api_tests
        [ "$run_performance" = true ] && run_performance_tests
        [ "$run_security" = true ] && run_security_tests
        [ "$run_load" = true ] && run_load_tests
    fi
    
    # Gerar relat√≥rios e limpar
    generate_reports
    cleanup
    
    success "Execu√ß√£o de testes conclu√≠da! üéâ"
    
    # Mostrar resumo
    echo ""
    log "üìä Resumo:"
    echo "  - Relat√≥rios: reports/"
    echo "  - Cobertura HTML: htmlcov/index.html"
    echo "  - Logs de teste: pytest.log"
    echo ""
    
    # Verificar se todos os testes passaram
    if [ -f "reports/test-results-unit.xml" ]; then
        if grep -q 'failures="0"' reports/test-results-unit.xml; then
            success "Todos os testes passaram! ‚úÖ"
        else
            error "Alguns testes falharam! ‚ùå"
            exit 1
        fi
    fi
}

# Executar fun√ß√£o principal
main "$@"
